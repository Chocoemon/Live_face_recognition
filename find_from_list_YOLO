import os
import signal
import cv2
import numpy as np
import asyncio
import io
import glob
import os
import sys
import time
import uuid
import requests
from urllib.parse import urlparse
from io import BytesIO
from PIL import Image, ImageDraw
from azure.cognitiveservices.vision.face import FaceClient
from msrest.authentication import CognitiveServicesCredentials
from azure.cognitiveservices.vision.face.models import TrainingStatusType, Person, SnapshotObjectType, OperationStatusType

#1. azure api에 접근하기 위한 object들을 key값을 이용해 생성해줌
KEY = os.environ['FACE_SUBSCRIPTION_KEY']
ENDPOINT = os.environ['FACE_ENDPOINT']

#2. Load Yolo( yolo 알고리즘 동작에 필요한 weight 파일들을 불러옴)
net = cv2.dnn.readNet("./data/weights/yolov3-wider_16000.weights", "./data/cfg/yolov3-face.cfg")
classes = []
with open("./data/names/coco.names", "r") as f:
    classes = [line.strip() for line in f.readlines()]
layer_names = net.getLayerNames()
output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]
colors = np.random.uniform(0, 255, size=(len(classes), 3))

# 이미지 이름을 변경해주기 위한 count 변수
count = 0
#3. 특정인을 찾기위해 특정인 사진을 입력받음
face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY))

# Display the detected face ID in the first single-face image.
#3-1 IDs are used for comparison to faces (their IDs) detected in other images.

# 해당 이미지에서 검출된 얼굴들이 모두 저장됨. (face라는 객체에 face_id라는 string 변수가 잇는듯. detected_face = list )
#3-2 Save this ID to use in Find Similar -> 이 코드는 id 한개만 저장

#4. load video
cap = cv2.VideoCapture("test (2).mp4")
cv2.dnn.DNN_TARGET_OPENCL

# 이건 반복문. 영상이 살아있거나, 특정 키를 누르기 전까지 계속 찾는걸 반복함.
while True:
    # capture video
    ret, frame = cap.read()
    key = cv2.waitKey(33)  # 1) & 0xFF
    # 현재 프레임 == 총 프레임 : 비디오의 끝 종료
    if cap.get(cv2.CAP_PROP_POS_FRAMES) == cap.get(cv2.CAP_PROP_FRAME_COUNT):
        print("video end")
        break
    # ctrl+z 이면, 특정 프레임에 검출된 사람들 중 similar 한 사람 있는지 검색

    # 프레임 수 조절 (Test 중에는 안씀)
    #if cap.get(cv2.CAP_PROP_POS_FRAMES) % 15 != 0:
        #continue
    frame = cv2.resize(frame, None, fx=1, fy=1)
    height, width, channels = frame.shape
    #5. Detecting objects
    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)

    net.setInput(blob)
    outs = net.forward(output_layers)

    # Showing information on the screen
    class_ids = []
    confidences = []
    boxes = []
    for out in outs:
        for detection in out:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5:
                # Object detected
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)

                # Rectangle coordinates
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)

                boxes.append([x, y, w, h])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

    path = "./outputs/"

    # print(indexes) 이런거 저장됏다고 보여주는 구문
    font = cv2.FONT_HERSHEY_PLAIN
    for i in range(len(boxes)):
        if i in indexes:
            x, y, w, h = boxes[i]
            # 박스 태그 (지우고 돌릴 것)
            #label = str(count)
            # 파일 이름
            name = str(count) + ".jpg"
            # print(label, "(", x, y, ")", ",", "(", x + w, y + h, ")")
            color = colors[i]
            #cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
            #cv2.putText(frame, label, (x, y + 30), font, 1, color, 1)

            # 얼굴 자른 후 outputs 폴더에 저장
            faceimg = frame[y - 10:y + h + 10, x - 10:x + w + 10]
            faceimg = cv2.resize(faceimg, dsize=(200, 200), interpolation=cv2.INTER_CUBIC)
            #cv2.imshow("linear", faceimg)
            #cv2.waitKey(0)
            cv2.destroyAllWindows()
            cv2.imwrite(path + name, faceimg)
            #cv2.imshow("Image", frame)
            count += 1
            print(count)

            # 6. 자른 얼굴을 API에 넣어 동일인인지 아닌지 검사
            multi_face_image_path = path+name
            multi_image_name = os.path.basename(multi_face_image_path)
            image_name_2 = open(multi_face_image_path, 'rb')
            detected_faces2 = face_client.face.detect_with_stream(image=image_name_2, return_face_id=True,
                                                                  recognition_model='recognition_03')

            if detected_faces2:
                # Display the detected face ID in the first single-face image.
                # Face IDs are used for comparison to faces (their IDs) detected in other images.
                print('Detected face ID from', multi_image_name, ':')
                for face in detected_faces2: print(face.face_id)

                # Search through faces detected in group image for the single face from first image.
                # First, create a list of the face IDs found in the second image.
                # second_image_face_IDs =  다수의 ID를 LIST의 형태로 받음detected_face list에서 face_id 리턴받음세
                second_image_face_IDs = list(map(lambda x: x.face_id, detected_faces2))
                # Next, find similar face IDs like the one detected in the first image.
                similar_faces = face_client.face.find_similar(face_id=face.face_id,face_list_id='test_list')
                for i in similar_faces:
                    print('Similar faces found in', multi_image_name + ':')
                    for face in similar_faces:
                        first_image_face_ID = face.face_id
                        # The similar face IDs of the single face image and the group image do not need to match,
                        # they are only used for identification purposes in each image.
                        # The similar faces are matched using the Cognitive Services algorithm in find_similar().
                        print(similar_faces[0].confidence)
                        img = cv2.imread(multi_face_image_path)
                        img = img[x - 10:x + w + 10, y - 10:y + h + 10]
                        #cv2.imshow("linear", img)
                        cv2.waitKey(0)
    key = cv2.waitKey(1) & 0xFF
    if key == ord("q"):
        break

cv2.destroyAllWindows()
cap.release()
print("program end")
os.kill(os.getpid(), signal.SIGTERM)
